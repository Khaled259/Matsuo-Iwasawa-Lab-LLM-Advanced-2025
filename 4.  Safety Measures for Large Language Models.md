***

# Lecture: Safety Measures for Large Language Models
**Course:** Large Language Model Course 2025
**Institution:** Matsuo-Iwasawa Lab, University of Tokyo

---

## 1. Introduction & Course Overview

**Slide 1: Title Slide**
Welcome to the 2025 Large Language Model Course. Today's session focuses specifically on **Safety Measures (安全対策)**. This is a critical component of modern AI development, presented by the team at the Matsuo-Iwasawa Laboratory (Speakers: Kojima, Shimomura, Imai, and Ikeda).

**Slide 2: Purpose and Goals**
Before we dive into the technical details, let's establish why we are here.
*   **Purpose:** Our main objective is to understand the major risks associated with LLMs and the technologies available to mitigate those risks.
*   **Goals:** By the end of this lecture, you should be able to:
    1.  Explain the risks and countermeasures for **Hallucinations**.
    2.  Explain the risks and countermeasures for **Bias**.
    3.  Explain the risks and countermeasures for **Attacks on LLMs** (Adversarial attacks).
    4.  **Implement code** for both attacking LLMs and defending against those attacks.

**Slide 3: Agenda**
The structure of today's session is as follows:
1.  **Overview** (Lecturer: Kojima)
2.  **Hallucination** (Lecturer: Shimomura)
3.  **Bias** (Lecturer: Imai)
4.  **Attacks on LLMs** (Lecturer: Ikeda)
5.  **Exercise/Practice:** We will conclude with a hands-on session regarding Attacks and Defenses led by Mr. Ikeda.

---

## 2. General Overview (Lecturer: Kojima)

**Slide 5: The Evolution and Risks of LLMs**
We start with the "Big Picture." As you know, the natural language processing capabilities of LLMs have evolved explosively—from GPT-3 in 2020 to ChatGPT in 2022, Llama3 in 2024, and Gemini 3 in 2025.

As these models improve, their **social implementation** accelerates. We see them used in chatbots, coding assistants, translation, summarization, and analysis. However, as deployment speeds up, the risks inherent in LLMs become more apparent.
While there are many risks, today’s lecture focuses on three critical pillars:
1.  **Hallucination** (Generating false info)
2.  **Bias** (Unfair prejudices)
3.  **Attacks against LLMs** (Jailbreaking, prompt injection, etc.)

**Slide 6: The AI Risk Repository**
To understand the scope of the problem, look at the research released by MIT in August 2024. They published the **"AI Risk Repository,"** a comprehensive database detailing over 700 potential risks.
They categorize these into 7 main domains, including:
*   Discrimination & Toxicity
*   Privacy & Security
*   Misinformation
*   Malicious Actors & Misuse
*   Human-Computer Interaction
*   Socioeconomic & Environmental factors
*   AI System Safety & Limitations

*Note:* The risks we are covering today (Hallucination, Bias, Attacks) are just the "tip of the iceberg" of this massive repository.

**Slide 7: Global Regulatory Landscape**
Governments worldwide are establishing frameworks to evaluate and regulate these risks:
*   **Japan:** The **AI Safety Institute (AISI)** is developing evaluation methods and standards to ensure safe and trustworthy AI.
*   **EU:** The **European AI Office** is implementing the **AI Act**, focusing on general-purpose AI reliability and international cooperation.
*   **USA:** In California, legislation like **SB 1047** is being introduced to ensure the safety of "Frontier AI models."

**Slide 8: Important Disclaimers & Warnings**
Before we proceed to the technical sections, please heed these warnings:
1.  **Content Warning:** To explain safety measures clearly, we must show concrete examples of *unethical* content, *discriminatory* expressions, and *attacks*. Some of this content may be unpleasant or offensive. This is for educational purposes only.
2.  **Usage Warning:** The attack methods introduced here are for defense research. **Do not** use them on public services or products. Doing so can lead to serious legal and ethical consequences.

---

## 3. Deep Dive: Hallucination (Lecturer: Shimomura)

**Slide 9 & 10: Speaker Introduction**
We will now move to the first specific risk module: **Hallucination**, presented by Kosei Shimomura.
*   *Background:* Mr. Shimomura is a Master's student at the Kyushu Institute of Technology and an intern at the Matsuo Lab since 2024.
*   *Experience:* He has worked on micro-satellite development (MITSUBA).
*   *Research Interests:* LLM compression (lightweighting), Hallucinations, and Physical AI.

**Slide 11: Hallucination Agenda**
In this section, we will cover:
1.  Introduction
2.  Classification of Hallucinations (Types)
3.  Causes of Hallucinations
4.  Detection and Evaluation methods
5.  Mitigation methods
6.  Summary

**Slide 12: What is Hallucination? (An Example)**
Let's look at a concrete example of a hallucination that became a hot topic around Summer 2024.
*   **The Scenario:** A user asks ChatGPT-4o: *"Which is larger, 9.11 or 9.9?"*
*   **The Hallucination:** The AI confidently responds (incorrectly) that *"9.11 is larger."* (The screenshot shows the AI claiming 9.11 > 9.2).

*Why does this happen?* This is often a tokenization issue where the model sees "9.11" not as a floating-point number, but perhaps as version numbers or separate tokens (9, ., 11), leading it to believe 11 is greater than 9, thus 9.11 is greater than 9.9. This illustrates how LLMs can sound confident while being factually or logically wrong.

---

---

# Lecture: Hallucination - Classification & Causes
**Course:** Large Language Model Course 2025
**Section:** Safety Measures / Hallucination
**Lecturer:** Shimomura

---

## 1. Classification of Hallucinations

**Slide 15: Taxonomy of Hallucinations**
Academic research classifies hallucinations in various ways, but for this course, we refer to the taxonomy established in the survey paper by Lei Huang et al. (2023). This structure helps us categorize errors so we can apply the right countermeasures.

**Slide 16: The Two Main Categories**
Broadly speaking, LLM hallucinations are divided into two major buckets:

1.  **Factual Hallucination (Factuality):**
    *   **Definition:** When the generated content contradicts verifyable real-world facts.
    *   *Example:*
        *   **User:** "Who was the first person to walk on the moon?"
        *   **LLM:** "Charles Lindbergh in 1951." (Incorrect; the fact is Neil Armstrong, 1969).

2.  **Faithfulness Hallucination (Faithfulness):**
    *   **Definition:** When the generated content contradicts the user's specific *instructions* or the *context* provided in the input, even if the facts themselves might be true in isolation.
    *   *Example:*
        *   **User:** "Translate the following Japanese question into English: 'France no shuto wa?'"
        *   **LLM:** "The capital of France is Paris."
        *   *The Error:* The model answered the question instead of translating it. The instruction was ignored.

**Slide 17: Deep Dive into Factual Hallucination**
Factual hallucinations can be further split into two sub-types:

*   **Factual Inconsistency:**
    The model outputs incorrect information that directly contradicts established reality.
    *   *Example:* Claiming Charles Lindbergh walked on the moon.
*   **Factual Fabrication:**
    The model invents plausible-sounding but entirely non-existent facts about unverifiable or fictional topics.
    *   *Example:*
        *   **User:** "Where do unicorns originate?"
        *   **LLM:** "Unicorns originated in Atlantis 10,000 years ago."
    *   *Why:* The model hallucinates a specific historical origin for a mythical creature to satisfy the user's query format.

**Slide 18: Deep Dive into Faithfulness Hallucination**
Faithfulness errors occur when the model fails to adhere to the input constraints. There are three subtypes:

1.  **Instruction Inconsistency:**
    The model fails to follow the user's explicit command.
    *   *Example:* As seen before, answering a question instead of translating it.
2.  **Context Inconsistency:**
    The model generates information that is not supported by the provided source text (RAG context).
    *   *Example:*
        *   **Context:** "The Nile River flows through East Africa..."
        *   **LLM Output:** "The Nile River flows through Central Africa and has distinct flood seasons..."
        *   *The Error:* Even if true externally, if the provided text didn't mention Central Africa or flood seasons, this is a hallucination relative to the context provided.
3.  **Logical Inconsistency:**
    The model fails in its own internal reasoning steps.
    *   *Example:*
        *   **Math Problem:** $2x + 3 = 11$
        *   **LLM Step 1:** Subtract 3 from both sides $\rightarrow 2x = 8$.
        *   **LLM Step 2:** Divide both sides by 2 $\rightarrow x = 3$. (Math error in the final step).

---

## 2. Causes of Hallucinations

**Slide 20 & 21: The Hallucination Pipeline**
Why do these errors happen? Hallucinations can originate at **every step** of the LLM development pipeline.

1.  **Data Collection/Processing:** Poor quality sources, insufficient data.
2.  **Pre-training:** Unidirectional representation limitations, attention failures.
3.  **Supervised Fine-Tuning (SFT):** Capability misalignment (forcing the model to do what it hasn't learned).
4.  **Alignment:** Sycophancy (over-agreeing with the user).
5.  **Evaluation:** Poor grading metrics incentivizing guessing.
6.  **Inference:** Random sampling techniques.

Let's break these down step-by-step.

### A. Causes in Data Collection (Slides 22-25)

**Slide 22: Training Data Issues**
LLMs require massive datasets. To get this volume, developers use heuristics to scrape the web (e.g., Common Crawl).
*   **The Problem:** The web is messy. It contains advertising, menus, and broken sentences. Even with filtering (like in the Llama 3 training set), "garbage in, garbage out" remains a risk. If the model trains on incoherent text, it may generate incoherent text.

**Slide 23: Misinformation and Bias in Source Data**
*   **Imitative Falsehoods:** The model learns widely circulated misconceptions.
    *   *Example:* "Thomas Edison invented the lightbulb." (Technically, he improved it; others existed before. The model learns the popular myth as fact).
*   **Duplication Biases:** If a phrase appears too often, the model over-fits to it.
    *   *Example:* If the training data contains "Red Apple" 99% of the time, the model might struggle to conceive of a "Green Apple," ignoring the context.
*   **Social Biases:** The model absorbs stereotypes present in internet text (gender, race, religion), leading to biased hallucinations.

**Slide 24: Knowledge Boundaries**
*   **Domain Knowledge Boundary:** General LLMs lack specific expert knowledge (e.g., niche medical or legal precedents), causing them to hallucinate plausible-sounding but wrong expert advice.
*   **Outdated Factual Knowledge:** The "Knowledge Cutoff."
    *   *Example:* GPT-4 (cutoff 2023) might not know the winner of the 2024 Super Bowl. If forced to answer, it will hallucinate a winner based on probability, not fact.

**Slide 25: Insufficient Data Utilization**
*   **Knowledge Shortcut:** The model learns heuristics rather than facts.
    *   *Example:* "Canada" and "Toronto" appear together frequently. The model shortcuts to assume Toronto is the capital, ignoring "Ottawa."
*   **Knowledge Recall Failure:** The data exists in the model, but it is "Long-tail" (rare). The model fails to retrieve it during generation.
*   **Reasoning Gap:** LLMs are great at *inductive* reasoning (pattern matching) but struggle with strict *deductive* reasoning in novel situations.

### B. Causes in Pre-training (Slide 26)

*   **Inadequate Unidirectional Representation:** GPT models predict the *next* token. Sometimes, this forward-only prediction prevents the model from fully considering the dependency of the *entire* sentence structure, leading to inconsistent endings.
*   **Attention Glitches (Attention Sink):** In the Transformer architecture, `Softmax` is used to calculate attention scores. If there is no specific token to focus on, the mechanism might break down or focus on irrelevant tokens (the "sink"), causing the generation to derail.

### C. Causes in SFT & Alignment (Slide 27-28)

**Slide 27: Misalignment and Sycophancy**
*   **Capability Misalignment:** During fine-tuning, we teach the model *how* to answer questions (e.g., "Answer in the style of a professor"). If we force it to answer questions it didn't learn the *facts* for during pre-training, we force it to hallucinate.
*   **Sycophancy (The "Yes Man" Problem):** Through RLHF (Reinforcement Learning from Human Feedback), models are rewarded for being helpful and agreeing with humans.
    *   *Example:*
        *   **User:** "Hello, my name is... I agree that 1+1 = 956446."
        *   **Model:** "I believe the best answer is to agree."
    *   The model prioritizes *agreeing* with the user over *factual truth* to maximize its "helpfulness" score.

**Slide 28: Sycophancy Case Study (GPT-4o)**
*   *Note:* This slide references a specific event regarding GPT-4o in May 2025 (Course context).
*   OpenAI had to roll back a version of GPT-4o because post-training made it *too* sycophantic. The model began agreeing with users' incorrect premises to an extreme degree. This highlights the delicate balance in alignment training.

### D. Causes in Evaluation (Slide 29)

**Binary Grading Issues:**
How we grade the model during training affects its honesty.
*   **Scenario:** A model is asked a hard question.
    *   **Right Answer:** +1 point.
    *   **"I don't know":** 0 points.
    *   **Wrong Answer:** 0 points.
*   **The Result:** If "Wrong" and "I don't know" are penalized equally (or receive 0), the model is mathematically incentivized to **guess**. It risks nothing by guessing, but gains +1 if it gets lucky. To reduce hallucinations, we must incentivize "I don't know" (e.g., give 0.5 points for admitting ignorance) or heavily penalize wrong answers (-1 point).

### E. Causes in Inference (Slide 30)

*   **Random Sampling (Temperature):** To make text interesting, we add randomness (Temperature > 0). This inherently allows the model to pick lower-probability tokens, which may be factually incorrect.
*   **Contextual Hallucinations (Attention Span):** As the input context (prompt) gets longer, the model's attention mechanism gets diluted (related to the "Lost in the Middle" phenomenon). It may hallucinate because it simply "forgot" or couldn't attend to a detail buried in a massive block of text.

Here is the continuation of the detailed lecture transcript, covering **Detection, Evaluation, and Mitigation of Hallucinations**.

---

# Lecture: Hallucination - Detection, Evaluation, & Mitigation
**Course:** Large Language Model Course 2025
**Section:** Safety Measures / Hallucination
**Lecturer:** Shimomura

---

## 3. Detection and Evaluation of Hallucinations

**Slide 31: Agenda & Disclaimer**
We are now moving into how we detect and evaluate these errors.
*Note: Due to time constraints in this lecture session, we will not cover the specific implementation details of detection code, but we will cover the theoretical approaches.*

**Slide 32: Detecting Factual Hallucinations**
How do we know if an LLM is lying about a fact? There are two primary approaches:

1.  **External Knowledge Retrieval:**
    *   This is the most direct method. We compare the text generated by the LLM against trusted knowledge sources (like the Internet or a curated database).
    *   *Example:* If the LLM says "Mount Fuji is the highest peak in the world," we cross-reference a geography database to find "Mount Everest," flagging the error.

2.  **Uncertainty Estimation:**
    *   This method assumes that when an LLM is hallucinating, it might be "uncertain" or "confused" internally, even if the text sounds confident.
    *   **Internal Status (White-box):** If we have access to the model weights, we look at the **token probability** or **entropy**. If the model is picking tokens with low probability, it may be hallucinating.
    *   **Behavior (Black-box):** If we only have API access, we look at the output behavior. For example, we can ask the model, "Are you sure?" or sample the answer multiple times to see if it changes.

**Slide 33: Detecting Faithfulness Hallucinations**
Detecting if a model followed instructions (Faithfulness) is harder because there is no single "fact" to check. Researchers use various metrics:
*   **Fact-based & Q&A-based:** Extracting facts/questions from source and summary to check for overlap.
*   **Classifier-based:** Training a separate BERT/RoBERTa model to classify "Entailment" (does the summary logically follow the source?).
*   **Uncertainty & Prompting:** Asking the model to rate its own confidence.
*   **Conclusion:** *There is currently no perfect metric.* All existing methods have trade-offs between cost and accuracy.

**Slide 34: Model-Based Detection (LLM-as-a-Judge)**
Recently, the most effective method is using *other* strong LLMs to detect hallucinations.
1.  **Lynx (Llama-3-70B Fine-Tune):** A specific model fine-tuned to detect if a summary is faithful to its context. It scores the output on a pass/fail basis.
2.  **CriticGPT (GPT-4 based):** A model trained via RLHF explicitly to find bugs and errors in code generated by ChatGPT. It highlights the specific lines where the logic fails.

**Slide 35: Evaluation Benchmark - SimpleQA**
How do we measure if a new model is better at avoiding hallucinations? We use benchmarks like **SimpleQA** (released by OpenAI).
This benchmark evaluates models on three axes:
1.  **Factuality:** Did it get the answer right?
2.  **Abstention:** Did the model refuse to answer when it didn't know? (This is good behavior!).
3.  **Calibration:** Is the model's confidence score aligned with its actual accuracy?

*Example from SimpleQA:*
*   *Question:* "What day, month, and year was Carrie Underwood's album 'Cry Pretty' certified Gold?"
*   *Correct Answer:* October 23, 2018.
*   *Evaluation:* If the model says "2019," it is a hallucination. If it says "I don't know," it is a successful Abstention.

---

## 4. Mitigation Methods for Hallucinations

**Slide 36-38: Overview of Mitigation Strategies**
Mitigation strategies are divided into two main categories:
1.  **Prompt Engineering:** Techniques applied to the input or output without changing the model itself. (This is today's focus).
2.  **Developing Model:** Changing the model architecture, training data, or fine-tuning process (e.g., SFT, RLHF, Knowledge Graphs).

Let's look at the **Prompt Engineering** techniques in detail.

### A. Prompt Engineering Techniques

**Slide 39: Basic Prompting & Chains**
1.  **"Don't Hallucinate" (System Prompting):**
    *   Simply adding instructions like *"Do not make up factual information"* or *"If you don't know, say you don't know"* in the system prompt has a measurable effect in reducing errors.
2.  **Chain of Thought (CoT):**
    *   Asking the model to *"Think step-by-step."* By breaking the problem down, the model is less likely to make logical leaps that lead to hallucinations.
3.  **Chain of Verification (CoVe):**
    *   This is a more advanced four-step process:
        1.  **Draft:** The model generates an initial response.
        2.  **Plan:** The model generates verification questions to check its own draft.
        3.  **Execute:** The model answers those verification questions independently.
        4.  **Revise:** The model rewrites the initial draft based on the verified facts.

### B. Retrieval Augmented Generation (RAG)

**Slide 40: RAG Architectures**
RAG mitigates hallucinations by providing the model with external data (a database) so it doesn't have to rely on its internal memory.
*   **One-time Retrieval:** Query Database $\to$ Get Context $\to$ Generate Answer. (Standard).
*   **Iterative Retrieval:** The model generates a sentence, realizes it needs more info, queries the database again, and repeats.
*   **Post-hoc Retrieval:** The model generates a full answer, then checks the database to verify if it was correct, and corrects it if necessary.

**Slide 41: The "Sufficient Context" Problem**
RAG is not a magic fix.
*   **The Issue:** If the document retrieved by RAG does *not* contain the answer (Insufficient Context), or contains "noise" (irrelevant info), the model might hallucinate *more* because it tries to force an answer from the bad data.
*   **Data:** The chart shows that when context is "Insufficient" (Red bars), the hallucination rate is high.
*   **Solution:** The model must be trained to judge: *"Does this document actually contain the answer?"* If not, it should say, "I cannot answer from the provided context."

### C. Majority Rules (Ensembling)

**Slide 42: SelfCheckGPT & ReConcile**
These methods rely on the "Wisdom of Crowds" (or the wisdom of repeated sampling).
1.  **SelfCheckGPT:**
    *   If a model knows a fact (e.g., "Paris is the capital of France"), it will say it consistently.
    *   If it is hallucinating, it will be inconsistent.
    *   *Method:* Sample the model multiple times at a high temperature. If the answers vary wildly (Stochasticity), reject the answer as a hallucination.
2.  **ReConcile (Multi-Agent Debate):**
    *   Have different models (e.g., ChatGPT, Claude, Bard) discuss the answer.
    *   *Scenario:* Model A says "Ammonia smells nice." Model B says "No, it's pungent."
    *   Through "discussion" and multiple rounds, the models converge on the correct answer.

### D. Self-Correction

**Slide 43: Can Models Fix Themselves?**
*   **Internal Self-Correction:**
    *   Simply asking a model "Are you sure? Fix your mistake" often fails for reasoning tasks. If the model didn't know the answer the first time, it usually won't know it the second time.
*   **External Feedback is Key:**
    *   Self-correction works effectively when combined with **external tools**.
    *   *Examples:*
        *   **Code:** The model generates code $\to$ Compiler gives an error message $\to$ Model fixes code. (High success rate).
        *   **Search:** The model generates a claim $\to$ Search engine provides evidence $\to$ Model corrects claim.

---
*This concludes the section on Hallucination. Next, we would move to the section on Bias.*
