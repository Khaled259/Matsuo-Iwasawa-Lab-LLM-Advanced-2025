{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "lcWCr0V9vqIE"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fdfb289056ea40188723a842e398449b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c52693067ee74eaba76048ee4262e158",
              "IPY_MODEL_853f8c4ea8e9403cae177526abf375d4",
              "IPY_MODEL_2cb9aea3d44a4685af34d442c0c95dee",
              "IPY_MODEL_a74e6928bbe4413b98227e5fdcb7834e",
              "IPY_MODEL_cf0515c540fe4710ac8d819877776ebe"
            ],
            "layout": "IPY_MODEL_12806db0f648477f82ba864832e47767"
          }
        },
        "c52693067ee74eaba76048ee4262e158": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c4569313213b42ba85d49a9a3ef1a397",
            "placeholder": "​",
            "style": "IPY_MODEL_a62db02611994a24bd7d3f7242a47015",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "853f8c4ea8e9403cae177526abf375d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_05b363d083a5437386b44596337fc0fd",
            "placeholder": "​",
            "style": "IPY_MODEL_ad0aa1850bdc46e0b188e867e7ffc351",
            "value": ""
          }
        },
        "2cb9aea3d44a4685af34d442c0c95dee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_b695627c9cc0493884b1964fb83dcb6c",
            "style": "IPY_MODEL_994165b021c44110af37608e54405f1e",
            "value": true
          }
        },
        "a74e6928bbe4413b98227e5fdcb7834e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_3b1df60a22464d298f71abe662d812cf",
            "style": "IPY_MODEL_370ed7b4f4ad45c8bf5a2cf436c7648d",
            "tooltip": ""
          }
        },
        "cf0515c540fe4710ac8d819877776ebe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_47e8e2c33e4a49d984a622d31a49d895",
            "placeholder": "​",
            "style": "IPY_MODEL_688e134c776c494fbb02dad777ef3057",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "12806db0f648477f82ba864832e47767": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "c4569313213b42ba85d49a9a3ef1a397": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a62db02611994a24bd7d3f7242a47015": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "05b363d083a5437386b44596337fc0fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad0aa1850bdc46e0b188e867e7ffc351": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b695627c9cc0493884b1964fb83dcb6c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "994165b021c44110af37608e54405f1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3b1df60a22464d298f71abe662d812cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "370ed7b4f4ad45c8bf5a2cf436c7648d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "47e8e2c33e4a49d984a622d31a49d895": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "688e134c776c494fbb02dad777ef3057": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Final Assignment (Main Competition) Inference Standard Code\n",
        "\n",
        "## 1. Overview\n",
        "This notebook contains standard code for generating benchmark inference results JSON and generating a competition submission JSON file using the LoRA adapter you trained and uploaded to Hugging Face.\n",
        "- The submission for the competition is not the trained LoRA itself, but the inference result JSON file.\n",
        "- This notebook provides instructions for reliably creating the submission JSON.\n",
        "\n",
        "- Generate (infer) answers to 150 questions sampled from StructEval-T.\n",
        "- Requires /content/public_150.json (handout) to run.\n",
        "- The output is inference.json (submission format), which you can upload to OmniCampus for grading."
      ],
      "metadata": {
        "id": "Yo1rCsGugXIw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Advance preparation"
      ],
      "metadata": {
        "id": "i86bwRT4iM1E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ":\n",
        "- Set the Colab runtime to **GPU (T4)**.\n",
        "- Log in to Hugging Face (token entry required).\n",
        "- As a general rule, the LoRA adapter used for inference will be the one uploaded in the learning notebook.\n",
        "\n",
        "--"
      ],
      "metadata": {
        "id": "P69owhiqiM3n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Execution procedure (recommended flow)"
      ],
      "metadata": {
        "id": "kK0KGIY3iM8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 0: Setup (clone / install)\n",
        "Execute the cells in order from top to bottom.\n",
        "\n",
        "- Clone `StructEval` and install dependencies (vLLM, etc.).\n",
        "- If `python3 -m structeval.cli --help` is displayed, the basic setup was successful.\n",
        "\n",
        "### Step 1: Hugging Face Login\n",
        "- Run `login()` and enter your token.\n",
        "\n",
        "### Step 2: LoRA Integration (Merge)\n",
        "- Load the LoRA at `adapter_id`, merge it with the base model, and generate `./merged_model`.\n",
        "- Once this is complete, use `./merged_model` as the model path for subsequent inferences.\n",
        "\n",
        "### Step 3: Run vLLM Inference and Generate Submission JSON\n",
        "- Generate `custom_inference.py`, and run it.\n",
        "- The inference results will be saved to `/content/StructEval/outputs/nonrenderable.json`.\n",
        "- Set `output` to `generation` and output the submission file `/content/inference.json`.\n",
        "- Download the output `/content/inference.json` and submit it to Omnicampus.\n",
        "--"
      ],
      "metadata": {
        "id": "awzqJJPIiM-_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Handling output files (submissions)"
      ],
      "metadata": {
        "id": "CnpX8ZJ7gzrF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Main Generated Files\n",
        "- Merged Model (No Submission Required)\n",
        "- `./merged_model/`\n",
        "\n",
        "- Inference Results **Submission File (Most Important)**\n",
        "- `/content/inference.json`\n",
        "- *This file has been formatted to include the `generation` field.\n",
        "\n",
        "### 4.2 Submission Procedure (Download → Upload to Omnicampus)\n",
        "1. In Colab, **download** the final output `/content/inference.json` to your local PC.\n",
        "- Open `/content/` from the \"Files\" (folder icon) on the left side of Colab.\n",
        "- Right-click `inference.json` → **Download**\n",
        "\n",
        "2. On the Omnicampus submission screen, **upload and submit** the downloaded `inference.json`.\n",
        "\n",
        "Please name the submission file `inference.json`.\n",
        "\n",
        "### **4.3 Important Points to Note when Participating in the Competition**:\n",
        "- For inference using this code, please use the \"trained and uploaded LoRA.\" Anyone submitting inference results using any other model will be disqualified.\n",
        "- The submission must be the \"inference result JSON\" (not the LoRA itself).\n",
        "- When submitting, be sure to include the URL of the adapter you uploaded to HuggingFace.\n",
        "\n",
        "--"
      ],
      "metadata": {
        "id": "Dzu1fQyiiyL5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Common mistakes and solutions"
      ],
      "metadata": {
        "id": "YBkvMWDCiyO8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **GPU is not enabled**\n",
        "- This may cause extremely slow inference or vLLM to fail. Be sure to check T4.\n",
        "\n",
        "- **`./merged_model` does not exist**\n",
        "- LoRA integration (merge) may not have completed. Please re-run the merge cell.\n",
        "\n",
        "- **Out of Memory (OOM) occurs when running vLLM**\n",
        "- This standard code uses `gpu_memory_utilization=0.6` for safety, but it may fail depending on the environment.\n",
        "- In this case, first restart the runtime (factory reset) and then re-run the same procedure.\n",
        "\n",
        "--"
      ],
      "metadata": {
        "id": "KDHi4u0ggztj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Expected final state (check)"
      ],
      "metadata": {
        "id": "uGKq9afckmbT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just before submission, the following conditions must be met:\n",
        "\n",
        "- `/content/inference.json` exists\n",
        "- The JSON is a list, and each element contains a `generation` field (it's not empty)\n",
        "- Upload `inference.json` to Omnicampus and submit it\n",
        "---"
      ],
      "metadata": {
        "id": "zMcE5FIVkmd9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Execution code\n"
      ],
      "metadata": {
        "id": "2LAE32yBP4Pr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 0: Setup (clone / install)"
      ],
      "metadata": {
        "id": "WJ8okWTTviHJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 0) Setup (Fixed version)\n",
        "\n",
        "\n",
        "!git clone -b fix-module-not-found-issue-2 https://github.com/Osakana7777777/StructEval.git\n",
        "\n",
        "!uv pip install \\\n",
        "  \"vllm==0.13.0\" \\\n",
        "  \"torch==2.9.0\" \\\n",
        "  \"torchaudio==2.9.0\" \\\n",
        "  \"torchvision==0.24.0\" \\\n",
        "  \"triton==3.5.0\" \\\n",
        "  \"compressed-tensors==0.12.2\" \\\n",
        "  \"openai==2.15.0\" \\\n",
        "  \"xgrammar==0.1.27\" \\\n",
        "  \"bitsandbytes==0.46.1\" \\\n",
        "  fire\n",
        "# Only flash-attn does not have a fixed version because its behavior changes depending on the environment\n",
        "!uv pip install flash-attn --no-build-isolation\n",
        "\n",
        "%cd StructEval\n",
        "!uv pip install -e .\n",
        "\n",
        "!python3 -m structeval.cli --help\n",
        "!mkdir -p outputs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGIUuwBZz9-o",
        "outputId": "e04f8c0d-8153-42fd-8bf1-547e4aad4099",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'StructEval'...\n",
            "remote: Enumerating objects: 17398, done.\u001b[K\n",
            "remote: Counting objects: 100% (149/149), done.\u001b[K\n",
            "remote: Compressing objects: 100% (123/123), done.\u001b[K\n",
            "remote: Total 17398 (delta 91), reused 45 (delta 26), pack-reused 17249 (from 3)\u001b[K\n",
            "Receiving objects: 100% (17398/17398), 529.90 MiB | 16.40 MiB/s, done.\n",
            "Resolving deltas: 100% (5424/5424), done.\n",
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m165 packages\u001b[0m \u001b[2min 1.92s\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mPrepared \u001b[1m47 packages\u001b[0m \u001b[2min 20.14s\u001b[0m\u001b[0m\n",
            "\u001b[2mUninstalled \u001b[1m4 packages\u001b[0m \u001b[2min 118ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m47 packages\u001b[0m \u001b[2min 172ms\u001b[0m\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1manthropic\u001b[0m\u001b[2m==0.71.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mapache-tvm-ffi\u001b[0m\u001b[2m==0.1.8.post2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mastor\u001b[0m\u001b[2m==0.8.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mbitsandbytes\u001b[0m\u001b[2m==0.46.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mblake3\u001b[0m\u001b[2m==1.0.8\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mcbor2\u001b[0m\u001b[2m==5.8.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mcompressed-tensors\u001b[0m\u001b[2m==0.12.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mdepyf\u001b[0m\u001b[2m==0.20.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mdiskcache\u001b[0m\u001b[2m==5.6.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mdnspython\u001b[0m\u001b[2m==2.8.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1memail-validator\u001b[0m\u001b[2m==2.3.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mfastapi-cli\u001b[0m\u001b[2m==0.0.20\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mfastapi-cloud-cli\u001b[0m\u001b[2m==0.11.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mfastar\u001b[0m\u001b[2m==0.8.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mfire\u001b[0m\u001b[2m==0.7.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mflashinfer-python\u001b[0m\u001b[2m==0.5.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mgguf\u001b[0m\u001b[2m==0.17.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mijson\u001b[0m\u001b[2m==3.4.0.post0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1minteregular\u001b[0m\u001b[2m==0.3.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mjmespath\u001b[0m\u001b[2m==1.1.0\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mlark\u001b[0m\u001b[2m==1.3.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mlark\u001b[0m\u001b[2m==1.2.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mllguidance\u001b[0m\u001b[2m==1.3.0\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mllvmlite\u001b[0m\u001b[2m==0.43.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mllvmlite\u001b[0m\u001b[2m==0.44.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mlm-format-enforcer\u001b[0m\u001b[2m==0.11.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mloguru\u001b[0m\u001b[2m==0.7.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmistral-common\u001b[0m\u001b[2m==1.8.8\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmodel-hosting-container-standards\u001b[0m\u001b[2m==0.1.13\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmsgspec\u001b[0m\u001b[2m==0.20.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mninja\u001b[0m\u001b[2m==1.13.0\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mnumba\u001b[0m\u001b[2m==0.60.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnumba\u001b[0m\u001b[2m==0.61.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cudnn-frontend\u001b[0m\u001b[2m==1.17.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cutlass-dsl\u001b[0m\u001b[2m==4.3.5\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mopenai-harmony\u001b[0m\u001b[2m==0.0.8\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1moutlines-core\u001b[0m\u001b[2m==0.2.11\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpartial-json-parser\u001b[0m\u001b[2m==0.2.1.1.post7\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mprometheus-fastapi-instrumentator\u001b[0m\u001b[2m==7.1.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpybase64\u001b[0m\u001b[2m==1.4.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpycountry\u001b[0m\u001b[2m==24.6.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpydantic-extra-types\u001b[0m\u001b[2m==2.11.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mray\u001b[0m\u001b[2m==2.53.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mrich-toolkit\u001b[0m\u001b[2m==0.17.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mrignore\u001b[0m\u001b[2m==0.7.6\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1msetproctitle\u001b[0m\u001b[2m==1.3.7\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1msetuptools\u001b[0m\u001b[2m==75.2.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1msetuptools\u001b[0m\u001b[2m==80.10.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1msupervisor\u001b[0m\u001b[2m==4.3.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mvllm\u001b[0m\u001b[2m==0.13.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mxgrammar\u001b[0m\u001b[2m==0.1.27\u001b[0m\n",
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m28 packages\u001b[0m \u001b[2min 1.05s\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 14.14s\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 3ms\u001b[0m\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mflash-attn\u001b[0m\u001b[2m==2.8.3\u001b[0m\n",
            "/content/StructEval\n",
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m235 packages\u001b[0m \u001b[2min 2.54s\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mPrepared \u001b[1m38 packages\u001b[0m \u001b[2min 1.19s\u001b[0m\u001b[0m\n",
            "\u001b[2mUninstalled \u001b[1m11 packages\u001b[0m \u001b[2min 33ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m38 packages\u001b[0m \u001b[2min 48ms\u001b[0m\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1maiodns\u001b[0m\u001b[2m==4.0.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1masttokens\u001b[0m\u001b[2m==3.0.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1masyncstdlib-fw\u001b[0m\u001b[2m==3.13.2\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mattrs\u001b[0m\u001b[2m==25.4.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mattrs\u001b[0m\u001b[2m==23.2.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mbackports-zstd\u001b[0m\u001b[2m==1.3.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mbetterproto-fw\u001b[0m\u001b[2m==2.0.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mblack\u001b[0m\u001b[2m==25.12.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mcolorama\u001b[0m\u001b[2m==0.4.6\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mdotenv\u001b[0m\u001b[2m==0.9.9\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1meval-type-backport\u001b[0m\u001b[2m==0.2.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mexecuting\u001b[0m\u001b[2m==2.2.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mfireworks-ai\u001b[0m\u001b[2m==0.19.20\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mhttpx-ws\u001b[0m\u001b[2m==0.8.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1micecream\u001b[0m\u001b[2m==2.1.10\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1minvoke\u001b[0m\u001b[2m==2.2.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mllm-engines\u001b[0m\u001b[2m==0.0.25\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmistralai\u001b[0m\u001b[2m==1.10.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmypy-extensions\u001b[0m\u001b[2m==1.1.0\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mopentelemetry-api\u001b[0m\u001b[2m==1.37.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-api\u001b[0m\u001b[2m==1.38.0\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mopentelemetry-exporter-otlp-proto-common\u001b[0m\u001b[2m==1.37.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-exporter-otlp-proto-common\u001b[0m\u001b[2m==1.38.0\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mopentelemetry-exporter-otlp-proto-http\u001b[0m\u001b[2m==1.37.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-exporter-otlp-proto-http\u001b[0m\u001b[2m==1.38.0\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mopentelemetry-proto\u001b[0m\u001b[2m==1.37.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-proto\u001b[0m\u001b[2m==1.38.0\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mopentelemetry-sdk\u001b[0m\u001b[2m==1.37.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-sdk\u001b[0m\u001b[2m==1.38.0\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mopentelemetry-semantic-conventions\u001b[0m\u001b[2m==0.58b0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-semantic-conventions\u001b[0m\u001b[2m==0.59b0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpathspec\u001b[0m\u001b[2m==1.0.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpdf2image\u001b[0m\u001b[2m==1.17.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mplaywright\u001b[0m\u001b[2m==1.57.0\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mprotobuf\u001b[0m\u001b[2m==5.29.5\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mprotobuf\u001b[0m\u001b[2m==5.29.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpycares\u001b[0m\u001b[2m==5.0.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpyee\u001b[0m\u001b[2m==13.0.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpytokens\u001b[0m\u001b[2m==0.4.0\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mrich\u001b[0m\u001b[2m==13.9.4\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mrich\u001b[0m\u001b[2m==14.2.0\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mruff\u001b[0m\u001b[2m==0.14.13\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mruff\u001b[0m\u001b[2m==0.9.10\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mstructeval\u001b[0m\u001b[2m==0.0.5 (from file:///content/StructEval)\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtogether\u001b[0m\u001b[2m==1.5.35\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mtyper\u001b[0m\u001b[2m==0.21.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtyper\u001b[0m\u001b[2m==0.19.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mwsproto\u001b[0m\u001b[2m==1.3.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mxmltodict\u001b[0m\u001b[2m==1.0.2\u001b[0m\n",
            "INFO: Showing help with the command 'cli.py -- --help'.\n",
            "\n",
            "\u001b[1mNAME\u001b[0m\n",
            "    cli.py\n",
            "\n",
            "\u001b[1mSYNOPSIS\u001b[0m\n",
            "    cli.py -\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Hugging Face Login\n",
        "- Run `login()` and enter your token."
      ],
      "metadata": {
        "id": "lcWCr0V9vqIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# -----------------------------\n",
        "# 1) HF login (once)\n",
        "# -----------------------------\n",
        "# Log in to HuggingFace to read the dataset on the HF Hub.\n",
        "#\n",
        "from huggingface_hub import login\n",
        "login()  # Colab will prompt"
      ],
      "metadata": {
        "id": "SrjUvUoPP6i9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436,
          "referenced_widgets": [
            "fdfb289056ea40188723a842e398449b",
            "c52693067ee74eaba76048ee4262e158",
            "853f8c4ea8e9403cae177526abf375d4",
            "2cb9aea3d44a4685af34d442c0c95dee",
            "a74e6928bbe4413b98227e5fdcb7834e",
            "cf0515c540fe4710ac8d819877776ebe",
            "12806db0f648477f82ba864832e47767",
            "c4569313213b42ba85d49a9a3ef1a397",
            "a62db02611994a24bd7d3f7242a47015",
            "05b363d083a5437386b44596337fc0fd",
            "ad0aa1850bdc46e0b188e867e7ffc351",
            "b695627c9cc0493884b1964fb83dcb6c",
            "994165b021c44110af37608e54405f1e",
            "3b1df60a22464d298f71abe662d812cf",
            "370ed7b4f4ad45c8bf5a2cf436c7648d",
            "47e8e2c33e4a49d984a622d31a49d895",
            "688e134c776c494fbb02dad777ef3057"
          ]
        },
        "outputId": "7133de96-0cba-4cc2-ea86-e2ff5ba1e9be"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fdfb289056ea40188723a842e398449b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: LoRA Merge\n",
        "- Load the LoRA at `adapter_id` and merge it with the base model to generate `./merged_model`.\n",
        "- Once this is complete, subsequent inference will use `./merged_model` as the model path."
      ],
      "metadata": {
        "id": "P8fExEKkwEtn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Now, upload \"public_150.json\" to the content folder.\n",
        "- You need to place the evaluation public_150.json in the Colab file area (/content)."
      ],
      "metadata": {
        "id": "xTCIiMYqe87c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------\n",
        "# 1) Config\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "MODEL_SOURCE = \"adapter_merge\"   # \"merged\" | \"base\" | \"adapter_merge\"\n",
        "# Select which model to use. For this example, select \"adapter_merge.\"\n",
        "\n",
        "# - \"base\": Base model (untrained raw model)\n",
        "# - \"merged\": Model with LoRA already merged (assuming it's distributed as a finished product)\n",
        "# - \"adapter_merge\": Load the base model and LoRA adapter on the fly and merge them locally before use.\n",
        "\n",
        "# base model (HF repo ID or local path)\n",
        "# Enter the base model used during training.\n",
        "BASE_MODEL_ID_OR_PATH   = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
        "\n",
        "# merged model (HF repo id or local path)\n",
        "# If you uploaded a merged model instead of an adapter, enter its ID here.\n",
        "# Fill in if you selected \"merged\"\n",
        "MERGED_MODEL_ID_OR_PATH = \"your_id/your-merged-repo\"\n",
        "\n",
        "# adapter merge\n",
        "# Enter the ID of the adapter you uploaded to HuggingFace.\n",
        "# Fill in if you selected \"adapter_merge\"\n",
        "ADAPTER_ID       = \"your_id/test-lora-repo\"\n",
        "# Temporarily save merged model\n",
        "MERGED_LOCAL_DIR = \"./merged_model\"\n",
        "\n",
        "# Specify input (150 questions) and output (submission) file paths\n",
        "INPUT_PATH  = \"/content/public_150.json\"\n",
        "OUTPUT_PATH = \"/content/inference.json\"\n",
        "\n",
        "\n",
        "TEMPERATURE = 0.0\n",
        "# 0.0 is the most deterministic (the same input is likely to produce the same output) and is generally stable for evaluation purposes.\n"
      ],
      "metadata": {
        "id": "5_F2mbjIwRoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Run vLLM inference and generate JSON for submission\n",
        "- `custom_inference.py` is generated and executed.\n",
        "- The inference results are saved to `/content/StructEval/outputs/nonrenderable.json`.\n",
        "- Set `output` to `generation` and output the submission file `/content/inference.json`.\n",
        "- Download the output `/content/inference.json` and submit it to Omnicampus.\n",
        "--"
      ],
      "metadata": {
        "id": "-O-8HtLKwRx5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ------------------------------------------------------------\n",
        "# 2) Stable vLLM env (IMPORTANT: must be set BEFORE importing vllm)\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "import os\n",
        "os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\"\n",
        "# The method of creating worker processes within vLLM will be fixed to \"spawn\".\n",
        "# In some environments, such as Colab, this may be more stable than \"fork\".\n",
        "\n",
        "os.environ[\"VLLM_LOGGING_LEVEL\"] = \"INFO\"\n",
        "# Set the vLLM log level (INFO). This is useful for debugging.\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 3) Resolve model_path\n",
        "# ------------------------------------------------------------\n",
        "# Depending on the MODEL_SOURCE you select, determine the \"model location (model_path)\" to be passed to vLLM.\n",
        "\n",
        "def resolve_model_path():\n",
        "    # A function that returns the path/ID to pass to vLLM depending on which model to use.\n",
        "    if MODEL_SOURCE == \"base\":\n",
        "        return BASE_MODEL_ID_OR_PATH\n",
        "\n",
        "    if MODEL_SOURCE == \"merged\":\n",
        "        return MERGED_MODEL_ID_OR_PATH\n",
        "\n",
        "    if MODEL_SOURCE == \"adapter_merge\":\n",
        "        # NOTE: To use torch/CUDA (GPU), do this before starting vLLM.\n",
        "        import os, gc\n",
        "        import torch\n",
        "        from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "        from peft import PeftModel\n",
        "        print(\"[INFO] Merging adapter into base model...\")\n",
        "        base_model = AutoModelForCausalLM.from_pretrained(\n",
        "            BASE_MODEL_ID_OR_PATH,\n",
        "            dtype=torch.float16,\n",
        "            device_map=\"auto\",\n",
        "            trust_remote_code=True,\n",
        "        )\n",
        "       # Load the tokenizer corresponding to the base model (usually the same one is used after merging)\n",
        "        tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID_OR_PATH, trust_remote_code=True)\n",
        "\n",
        "        # Merge the LoRA adapter (ADAPTER_ID) into the base_model.\n",
        "        # After merging, you can remove the LoRA layer (unload), simplifying inference handling.\n",
        "        model_to_merge = PeftModel.from_pretrained(base_model, ADAPTER_ID)\n",
        "        merged_model = model_to_merge.merge_and_unload()\n",
        "\n",
        "        os.makedirs(MERGED_LOCAL_DIR, exist_ok=True)\n",
        "        merged_model.save_pretrained(MERGED_LOCAL_DIR)\n",
        "        tokenizer.save_pretrained(MERGED_LOCAL_DIR)\n",
        "\n",
        "        del base_model, model_to_merge, merged_model\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "        print(\"[INFO] Merged model saved:\", MERGED_LOCAL_DIR)\n",
        "        return MERGED_LOCAL_DIR\n",
        "\n",
        "    raise ValueError(\"MODEL_SOURCE must be 'merged'|'base'|'adapter_merge'\")\n",
        "\n",
        "# Determine the path/ID of the final model to be used\n",
        "model_path = resolve_model_path()\n",
        "print(\"[INFO] Using model:\", model_path)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 4) Load public_150 and build prompts (no torch usage here)\n",
        "# ------------------------------------------------------------\n",
        "# Read the input file and create prompts (strings to pass to the model) for each question.\n",
        "\n",
        "import json\n",
        "from pathlib import Path\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "pub = json.loads(Path(INPUT_PATH).read_text(encoding=\"utf-8\"))\n",
        "\n",
        "assert isinstance(pub, list), \"public_150.json must be a list\"\n",
        "assert len(pub) == 150, f\"public_150 must have 150 items, got {len(pub)}\"\n",
        "assert len({x[\"task_id\"] for x in pub}) == 150, \"public_150 has duplicate task_id\"\n",
        "\n",
        "# Safety: ensure output_type exists (office enriched file)\n",
        "\n",
        "missing_ot = [x.get(\"task_id\") for x in pub if not (x.get(\"output_type\") or \"\").strip()]\n",
        "\n",
        "if missing_ot:\n",
        "    raise RuntimeError(f\"FATAL: public_150 missing output_type (not enriched). Examples: {missing_ot[:5]}\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
        "\n",
        "# task_ids: Stores the sequence of task_ids to be used for output.\n",
        "# prompts: Stores the prompt string to be passed to vLLM.\n",
        "task_ids, prompts = [], []\n",
        "\n",
        "for item in pub:\n",
        "    task_ids.append(item[\"task_id\"])\n",
        "    query = item.get(\"query\", \"\")\n",
        "    messages = [{\"role\": \"user\", \"content\": query}]\n",
        "    prompts.append(tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))\n",
        "    # ↑ Use apply_chat_template to format the string into the conversational format expected by the model.\n",
        "    # tokenize=False: Do not tokenize yet and return as a string.\n",
        "    # add_generation_prompt=True: Add a boundary where the assistant will answer.\n",
        "    # This makes it easier for the model to continue generating answers.\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 5) Presets + fallback plan\n",
        "# ------------------------------------------------------------\n",
        "# If you set the \"context length (max_model_len)\" or \"output limit (max_tokens)\" too large when starting vLLM,\n",
        "# it is likely to crash due to insufficient GPU memory (OOM).\n",
        "# Therefore, we prepare several settings that are likely to succeed, and if they fail, we gradually reduce the settings and retry.\n",
        "# Because actual memory usage can vary between merged (already baked) and adapter_merge (on-the-fly merge),\n",
        "# the settings to be tried first (e.g., gpu_mem) are different.\n",
        "# Create a \"trial candidate list\" in advance and try them in order from top to bottom.\n",
        "\n",
        "def build_try_configs():\n",
        "\n",
        "    # Primary presets\n",
        "\n",
        "    if MODEL_SOURCE == \"merged\":\n",
        "        base = [\n",
        "            {\"max_model_len\": 4096, \"max_tokens\": 4096, \"gpu_mem\": 0.85},\n",
        "            {\"max_model_len\": 4096, \"max_tokens\": 4096, \"gpu_mem\": 0.80},\n",
        "        ]\n",
        "       # ↑ Try increasing GPU usage from 0.85 to 0.80 while allowing up to 4096 tokens of context/output.\n",
        "    elif MODEL_SOURCE == \"adapter_merge\":\n",
        "        base = [\n",
        "            {\"max_model_len\": 4096, \"max_tokens\": 4096, \"gpu_mem\": 0.60},\n",
        "            {\"max_model_len\": 4096, \"max_tokens\": 4096, \"gpu_mem\": 0.65},\n",
        "        ]\n",
        "        # ↑ adapter_merge tends to be memory intensive, so try starting with a low gpu_mem.\n",
        "\n",
        "    else:  # base\n",
        "        base = [\n",
        "            {\"max_model_len\": 4096, \"max_tokens\": 4096, \"gpu_mem\": 0.80},\n",
        "            {\"max_model_len\": 4096, \"max_tokens\": 4096, \"gpu_mem\": 0.70},\n",
        "        ]\n",
        "        # ↑ The base model is assumed to be relatively light, so we will try 0.80→0.70.\n",
        "\n",
        "    # Fallback ladder (reduce context / output)\n",
        "    # A \"gradual reduction setting\" in case of failure.\n",
        "    # Lowering max_model_len and max_tokens reduces memory requirements and increases the likelihood of success.\n",
        "    ladder = [\n",
        "        {\"max_model_len\": 3072, \"max_tokens\": 3072},\n",
        "        {\"max_model_len\": 2048, \"max_tokens\": 2048},\n",
        "        {\"max_model_len\": 1536, \"max_tokens\": 1536},\n",
        "    ]\n",
        "\n",
        "  # Expand base configs with ladder and a couple of gpu_mem tweaks\n",
        "# ↑ \"Mix\" ladder steps into the base configuration to increase the number of trial patterns.\n",
        "# Also, try a version that slightly increases gpu_mem (this may be effective when the failure reason is \"insufficient memory allocation\").\n",
        "    out = []\n",
        "    for cfg in base:\n",
        "        out.append(cfg)\n",
        "\n",
        "        for step in ladder:\n",
        "            out.append({**cfg, **step})\n",
        "\n",
        "        # try a slightly higher gpu_mem if still failing (some failures are \"not enough alloc\")\n",
        "        out.append({**cfg, \"gpu_mem\": min(0.90, cfg[\"gpu_mem\"] + 0.05)})\n",
        "\n",
        "# Deduplicate while preserving order\n",
        "# ↑ Similar settings may overlap, so we'll delete them while preserving the order.\n",
        "    seen = set()\n",
        "    uniq = []\n",
        "    for c in out:\n",
        "        key = (c[\"max_model_len\"], c[\"max_tokens\"], round(c[\"gpu_mem\"], 2))\n",
        "\n",
        "        if key in seen:\n",
        "            continue\n",
        "\n",
        "        seen.add(key)\n",
        "        uniq.append(c)\n",
        "\n",
        "    return uniq\n",
        "\n",
        "\n",
        "TRY_CONFIGS = build_try_configs()\n",
        "# ↑ Create a list of settings to try out.\n",
        "\n",
        "print(\"[INFO] Try configs (in order):\")\n",
        "\n",
        "for i, c in enumerate(TRY_CONFIGS[:8], 1):\n",
        "    print(f\"  {i:02d}. max_model_len={c['max_model_len']} max_tokens={c['max_tokens']} gpu_mem={c['gpu_mem']}\")\n",
        "\n",
        "if len(TRY_CONFIGS) > 8:\n",
        "    print(f\"  ... total {len(TRY_CONFIGS)} configs\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 6) vLLM run with retry\n",
        "# ------------------------------------------------------------\n",
        "# ↑ This is the main part of the inference.\n",
        "\n",
        "from vllm import LLM, SamplingParams\n",
        "def run_with_config(cfg):\n",
        "\n",
        "    sampling = SamplingParams(\n",
        "        temperature=TEMPERATURE,\n",
        "        max_tokens=cfg[\"max_tokens\"],\n",
        "    )\n",
        "\n",
        "    llm = LLM(\n",
        "        model=model_path,\n",
        "        max_model_len=cfg[\"max_model_len\"],\n",
        "        gpu_memory_utilization=cfg[\"gpu_mem\"],\n",
        "        enforce_eager=True,\n",
        "        tensor_parallel_size=1,\n",
        "         disable_log_stats=True,\n",
        "    )\n",
        "\n",
        "    outs = llm.generate(prompts, sampling)\n",
        "\n",
        "    submission = []\n",
        "# ↑ Create a submission form [{\"task_id\": ..., \"generation\": ...}, ...].\n",
        "\n",
        "    for tid, out in zip(task_ids, outs):\n",
        "        gen = out.outputs[0].text if out.outputs else \"\"\n",
        "        submission.append({\"task_id\": tid, \"generation\": gen})\n",
        "    return submission\n",
        "# ↑ Returns a submission array for 150 questions.\n",
        "last_err = None\n",
        "submission = None\n",
        "# ↑ Variable to store submitted data (150 items) if successful. None until successful.\n",
        "for idx, cfg in enumerate(TRY_CONFIGS, 1):\n",
        "    print(f\"[INFO] Attempt {idx}/{len(TRY_CONFIGS)}: max_model_len={cfg['max_model_len']} max_tokens={cfg['max_tokens']} gpu_mem={cfg['gpu_mem']}\")\n",
        "    try:\n",
        "        submission = run_with_config(cfg)\n",
        "        print(\"[INFO] ✅ Generation succeeded with this config.\")\n",
        "        # ↑ Success log\n",
        "        break\n",
        "    except RuntimeError as e:\n",
        "        last_err = e\n",
        "        msg = str(e)\n",
        "        print(\"[WARN] Failed:\", msg[:200].replace(\"\\n\", \" \"))\n",
        "\n",
        "# try next config\n",
        "if submission is None:\n",
        "    raise RuntimeError(f\"All configs failed. Last error: {last_err}\")\n",
        "\n",
        "\n",
        "# Final guards\n",
        "# ↑ Finally, perform a \"submission consistency check.\"\n",
        "\n",
        "if len(submission) != 150:\n",
        "    # ↑ Check if 150 items have been generated\n",
        "    raise RuntimeError(f\"Submission count mismatch: {len(submission)}\")\n",
        "\n",
        "if len({x['task_id'] for x in submission}) != 150:\n",
        "   # ↑ Check for duplicate task_ids\n",
        "    raise RuntimeError(\"Duplicate task_id in submission\")\n",
        "\n",
        "Path(OUTPUT_PATH).write_text(json.dumps(submission, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "# ↑ Convert the submission (Python object) into a JSON string and save it to a file.\n",
        "\n",
        "print(\"[OK] wrote:\", OUTPUT_PATH, \"items=150\")\n"
      ],
      "metadata": {
        "id": "rCbWE8H6qDeS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}